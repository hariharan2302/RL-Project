{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariharan2302/RL-Project/blob/main/RVO2_Environment_for_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9V8f21M0Xk4"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import gymnasium as gym\n",
        "from gym import spaces\n",
        "import rvo2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def point_to_segment_dist(x1, y1, x2, y2, x3, y3):\n",
        "    \"\"\"\n",
        "    Calculate the closest distance between point(x3, y3) and a line segment with two endpoints (x1, y1), (x2, y2)\n",
        "\n",
        "    \"\"\"\n",
        "    px = x2 - x1\n",
        "    py = y2 - y1\n",
        "\n",
        "    if px == 0 and py == 0:\n",
        "        return np.linalg.norm((x3-x1, y3-y1))\n",
        "\n",
        "    u = ((x3 - x1) * px + (y3 - y1) * py) / (px * px + py * py)\n",
        "\n",
        "    if u > 1:\n",
        "        u = 1\n",
        "    elif u < 0:\n",
        "        u = 0\n",
        "\n",
        "    # (x, y) is the closest point to (x3, y3) on the line segment\n",
        "    x = x1 + u * px\n",
        "    y = y1 + u * py\n",
        "\n",
        "    return np.linalg.norm((x - x3, y-y3))\n",
        "\n",
        "class humans():\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "class Robot():\n",
        "    def __init__(self) -> None:\n",
        "        self.initial_pos = (5, 0)\n",
        "        self.goal_pos = (5, 10)\n",
        "        self.current_pos = self.initial_pos\n",
        "        self.velocity = (0, 0)\n",
        "        # self.get_current_pos()\n",
        "\n",
        "    def get_current_pos(self, velocity):\n",
        "        if self.current_pos is None:\n",
        "            self.current_pos = self.initial_pos\n",
        "        else:\n",
        "            ix, iy = self.current_pos\n",
        "            vx, vy = velocity\n",
        "            self.velocity = velocity\n",
        "            x, y = (ix + min(vx,5)*0.1, iy + min(vy,5)*0.1)\n",
        "            clipped_x = max(0, min(x, 10))\n",
        "            clipped_y = max(0, min(y, 10))\n",
        "            self.current_pos = (clipped_x, clipped_y)\n",
        "\n",
        "        self.v_pref = np.array(self.goal_pos) - np.array(self.current_pos)\n",
        "        return self.current_pos\n",
        "\n",
        "\n",
        "class Environment_Sim(gym.Env):\n",
        "    def __init__(self,number):\n",
        "        super(Environment_Sim, self).__init__()\n",
        "        self.humans_size = number\n",
        "        self.humans_pos = np.random.randint(1, 11, size=(self.humans_size, 2))\n",
        "        self.goal_pos = np.random.randint(1, 11, size=(self.humans_size, 2))\n",
        "        self.sim = rvo2.PyRVOSimulator(1/15, 1.5, 10, 1.5, 2, 0.4, 2)\n",
        "        self.humans = []\n",
        "        self.agents = [self.sim.addAgent(tuple(pos)) for pos in self.humans_pos]\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "\n",
        "        self.robot = Robot()\n",
        "        self.dones = np.array([False]*(self.humans_size+1))\n",
        "        self.time_step = 1/20\n",
        "\n",
        "        #action space\n",
        "        self.max_steps = 100\n",
        "        self.action_space = spaces.Discrete(9)\n",
        "        self.action_space_map = {\n",
        "            0: [0, 0],   # Stop moving\n",
        "            1: [1.7, 0],   # Right\n",
        "            2: [0, 1.7],   # Up\n",
        "            3: [-1.7, 0],  # Left\n",
        "            4: [0, -1.7],  # Down\n",
        "            5: [1.7, 1.7],   # Up-Right (Diagonal)\n",
        "            6: [-1.7, 1.7],  # Up-Left (Diagonal)\n",
        "            7: [1.7, -1.7],  # Down-Right (Diagonal)\n",
        "            8: [-1.7, -1.7]  # Down-Left (Diagonal)\n",
        "        }\n",
        "\n",
        "\n",
        "    def reward_system(self, robot, humans, action):\n",
        "        collision_penalty = -20\n",
        "        success_reward = 500\n",
        "        discomfort_penalty = -10\n",
        "\n",
        "        #Max steps reached\n",
        "        if self.max_steps:\n",
        "            self.max_steps -= 1\n",
        "        else :\n",
        "            self.max_steps = 100\n",
        "            self.dones[0] = True\n",
        "            return 0, True, \"Limit\"\n",
        "\n",
        "        # # Collision detection between robot and humans\n",
        "        # collision = False\n",
        "        # for human in humans:\n",
        "        #     if self.check_collision(robot, human, action):\n",
        "        #         collision = True\n",
        "        #         break\n",
        "\n",
        "        # Collision detection between robot and humans - 2\n",
        "        dmin = float('inf')\n",
        "        gdmin = 10\n",
        "        collision = False\n",
        "        for human in humans:\n",
        "            closest_dist = self.collision(robot, human, action)\n",
        "            if closest_dist < 0:\n",
        "                collision = True\n",
        "            elif closest_dist < dmin:\n",
        "                dmin = closest_dist\n",
        "\n",
        "        # Check for goal reaching\n",
        "        reached, distance, reward = self.is_goal_reached(robot, action)\n",
        "        if reached:\n",
        "            self.dones[0] = True\n",
        "            # return (success_reward/(100 - self.max_steps))**2, True, \"Reached Goal\"\n",
        "            return success_reward, True, \"Reached Goal\"\n",
        "        # Check for collisions\n",
        "        if collision:\n",
        "            self.dones[0] = True\n",
        "            return collision_penalty, True, \"Collision\"\n",
        "\n",
        "        if dmin < 0.2:\n",
        "            return -10, False, \"Risk for Collision\"\n",
        "\n",
        "        # if distance < gdmin:\n",
        "        #     reward = (10 - distance)/10\n",
        "        #     gdmin = distance\n",
        "        # else:\n",
        "\n",
        "        # x,y = self.robot.current_pos\n",
        "        # x1,y1 = self.robot.goal_pos\n",
        "        # reward = -np.sqrt((x-x1)**2+(y-y1)**2)\n",
        "        reward = -1*(distance)\n",
        "        return reward, False, \"Nothing\"\n",
        "\n",
        "    def check_collision(self, robot, human, action):\n",
        "        next_robot_position = np.array(robot.current_pos) + np.array(action) * self.time_step\n",
        "        next_human_position = np.array(self.sim.getAgentPosition(human)) + np.array(self.sim.getAgentVelocity(human)) * self.time_step\n",
        "        distance = np.linalg.norm(next_robot_position - next_human_position)\n",
        "        return distance < (0.4 + 0.4)\n",
        "\n",
        "    def collision(self, robot, human, action):\n",
        "        hpx, hpy = self.sim.getAgentPosition(human)\n",
        "        rpx, rpy = robot.current_pos\n",
        "        px = hpx - rpx\n",
        "        py = hpy - rpy\n",
        "\n",
        "        hvx, hvy = self.sim.getAgentVelocity(human)\n",
        "        rvx, rvy = action\n",
        "        vx = hvx - rvx\n",
        "        vy = hvy - rvy\n",
        "\n",
        "        ex = px + vx * self.time_step\n",
        "        ey = py + vy * self.time_step\n",
        "        # closest distance between boundaries of two agents\n",
        "        closest_dist = point_to_segment_dist(px, py, ex, ey, 0, 0) - 0.8\n",
        "        return closest_dist\n",
        "\n",
        "\n",
        "    def is_goal_reached(self, robot, action):\n",
        "        next_position = np.array(robot.current_pos) + np.array(action) * self.time_step\n",
        "        distance = np.linalg.norm(next_position - np.array(self.robot.goal_pos))\n",
        "        reward = np.linalg.norm(np.array(robot.current_pos) - np.array(self.robot.goal_pos))\n",
        "        return distance < 0.75, distance, reward\n",
        "\n",
        "    def obtain_joinSate(self):\n",
        "        # robot_state []\n",
        "        robot_pos = list(self.robot.current_pos)\n",
        "        robot_goal = list(self.robot.goal_pos)\n",
        "        robot_vpref = (robot_goal[0] - robot_pos[0], robot_goal[1] - robot_pos[1])\n",
        "        rvx, rvy = self.robot.velocity\n",
        "        # robot_state = [robot_pos[0], robot_pos[1], rvx, rvy, 0.4, robot_goal[0], robot_goal[1], robot_vpref[0], robot_vpref[1]]\n",
        "        robot_state = [robot_pos[0], robot_pos[1], rvx, rvy, robot_goal[0], robot_goal[1]]\n",
        "        # robot_state = torch.tensor(robot_state)\n",
        "\n",
        "        # human_state []\n",
        "        human_states = []\n",
        "        for human_idx in self.agents:\n",
        "            human_posX, human_posY = self.sim.getAgentPosition(human_idx)\n",
        "            human_goal = self.goal_pos[human_idx]\n",
        "            human_vpref = list(self.sim.getAgentPrefVelocity(human_idx))\n",
        "            hvx, hvy = human_vpref\n",
        "            # human_state = [human_posX, human_posY, hvx, hvy, 0.4, human_goal[0], human_goal[1], human_vpref[0], human_vpref[1]]\n",
        "            human_state = [human_posX, human_posY, hvx, hvy,human_goal[0], human_goal[1]]\n",
        "            human_states.append(human_state)\n",
        "\n",
        "        human_states.append(robot_state)\n",
        "        join_state = torch.cat([torch.tensor(state) for state in human_states])\n",
        "        return join_state.float()\n",
        "\n",
        "    def reset(self):\n",
        "        self.humans_pos = np.random.randint(1, 11, size=(self.humans_size, 2))\n",
        "        self.goal_pos = np.random.randint(1, 11, size=(self.humans_size, 2))\n",
        "        self.humans = []\n",
        "        self.sim = rvo2.PyRVOSimulator(1/15, 1.5, 10, 1.5, 2, 0.4, 2)\n",
        "        self.agents = [self.sim.addAgent(tuple(pos)) for pos in self.humans_pos]\n",
        "\n",
        "        self.robot = Robot()\n",
        "        self.dones = np.array([False]*(self.humans_size+1))\n",
        "\n",
        "        #action space\n",
        "        self.max_steps = 100\n",
        "        state = self.obtain_joinSate()\n",
        "        return state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        for agent, goal in zip(self.agents, self.goal_pos):\n",
        "            px, py = self.sim.getAgentPosition(agent)\n",
        "            velocity = (goal[0] - px, goal[1] - py)\n",
        "            self.sim.setAgentPrefVelocity(agent, velocity)\n",
        "            px, py = self.sim.getAgentPosition(agent)\n",
        "            gx, gy = self.goal_pos[agent]\n",
        "            if (gx-px) < 0.05 and (gy-py) < 0.05:\n",
        "                self.dones[agent+1] = True\n",
        "\n",
        "        self.sim.doStep()\n",
        "        reward, done = 0, 0\n",
        "\n",
        "        # print()\n",
        "        # self.render()\n",
        "\n",
        "        # print(self.reward_system(self.robot, self.agents, action))\n",
        "        action = tuple(self.action_space_map[action.item()])\n",
        "        reward, done, info = self.reward_system(self.robot, self.agents, action)\n",
        "        self.robot.get_current_pos(action)\n",
        "        state = self.obtain_joinSate()\n",
        "        # done = np.all(self.dones)\n",
        "        return state, reward, done, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        self.ax.clear()\n",
        "        self.ax.set_xlim(-1, 11)\n",
        "        self.ax.set_ylim(-1, 11)\n",
        "        for pos in [self.sim.getAgentPosition(agent) for agent in self.agents]:\n",
        "            self.ax.plot(pos[0], pos[1], 'bo')\n",
        "        # robot_posX, robot_posY = self.robot.initial_pos\n",
        "        # self.ax.plot(robot_posX, robot_posY, 'ro')\n",
        "        goal_posX, goal_posY = self.robot.goal_pos\n",
        "        self.ax.plot(goal_posX, goal_posY, 'yo', marker=\"*\",markersize=9)\n",
        "\n",
        "        # d = self.sim.getAgentPosition(self.r)\n",
        "        d = self.robot.current_pos\n",
        "        self.ax.plot(d[0], d[1], 'ro')\n",
        "        plt.pause(0.02)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     env = Environment_Sim(4)\n",
        "#     env.reset()\n",
        "#     done = False\n",
        "#     i = 0\n",
        "#     while not done:\n",
        "#         state, reward, done, done, info = env.step(2)\n",
        "#         i += 1\n",
        "#         env.render()\n",
        "#     print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QprVN8ov0Xk6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}